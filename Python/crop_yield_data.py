# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import geopandas as gpd
import netCDF4 as nc
import matplotlib.pyplot as plt
import os
import rasterio
from rasterio import features
import gzip
import xarray as xr
from rasterio.enums import Resampling
import pickle
import lzma
import feather as ft
import itertools
from copy import deepcopy


def import_analysis_unit(path, dims):
# Import FPU raster data from path (directory) and filename
    with rasterio.open(path) as aunit_raster:
        
        aunit_array = aunit_raster.read(
            out_shape = (aunit_raster.count, dims[0], dims[1]),\
            resampling = Resampling.nearest) \
            .squeeze()
        
        aunit_meta = aunit_raster.meta.copy()
    
    aunit_array[aunit_array <= 0] = 0
    aunit_array = aunit_array.astype(int)
    aunit_meta['nodata'] = 0
    return aunit_array, aunit_meta


def extract_data(file, path_in,data_source ="ray"):
    
    file = path_in + file
    
    if file.endswith('.gz'):
        unzipped_file = gzip.open(file)
        nc_data = nc.Dataset('dummy', mode='r', memory=unzipped_file.read())
    else:
        nc_data = nc.Dataset(file, mode = 'r')
    
    
    if data_source == "ray":
        lon_name = "longitude"
        lat_name = "latitude"
        data_name = "Data"
        lon_shift = 0
    elif data_source == "iizumi":
        lon_name = "lon"
        lat_name = "lat"
        data_name = "var"
        lon_shift = 180
    
    lat = nc_data[lat_name][:].squeeze()
    lon = nc_data[lon_name][:].squeeze()-lon_shift
    var = nc_data[data_name][:].squeeze()
    var[np.isnan(var)] = 0
    
    if 'areaweightedyield' in file:
        if "soybean" in file:
            var[var>15] = 0
        else:
            var[var > 25] = 0
    
    if lat[0] < lat[1]:
        lat = np.flip(lat)
        var = np.flip(var, axis = 0)
        
    if lon[0] > lon[1]:
        lon = np.flip(lon)
        var = np.flip(var, axis = 1)
    shift = False
    if shift == True:
        pass
    
    data_dict = {'data': var,
               'lat': lat,
               'lon': lon,
               'filename': file}
    return data_dict

def import_ray_crop_data(path_in, var_name, dtype_out ="array"):
    
    #os.chdir(path_in)
    # currently redundant
    years_select = [ str(val) for val in range(1970,2014)]
    file_list = [ path for year in years_select for path in os.listdir(path_in) if year in path]
    
    #file_list = [path for path in os.listdir(path_in)]
    file_list = sorted(file_list)
    
    data_out = [extract_data(file,path_in)['data'] for file in file_list if var_name in file]
    
    if dtype_out == "array":
        data_out = np.stack(data_out, axis =2)
        data_out[data_out == 0] = np.nan
    
    return data_out




def import_crop_data(path_in, var_name, year_range = range(1981,2014), dtype_out ="array"):
    
    # Ray et al. or Iizumi et al
    
    #os.chdir(path_in)
    years_select = [ str(val) for val in year_range]
    file_list = [ path for year in years_select for path in os.listdir(path_in) if year in path]
    
    #file_list = [path for path in os.listdir(path_in)]
    file_list = sorted(file_list)
    
    data_out = [extract_data(file,path_in)['data'] for file in file_list if var_name in file]
    
    if dtype_out == "array":
        data_out = np.stack(data_out, axis =2)
        data_out.set_fill_value(np.nan)

        data_out[data_out == 0] = np.nan
    
    return data_out


def test_fig(array_in):
    
    if array_in.ndim >2:
        nlyr = np.shape(array_in)[2]
    else:
        nlyr = 1
        array_in = array_in[:,:,np.newaxis]
    for ii in range(nlyr):
        plt.figure()
        plt.imshow(array_in[:,:,ii])
        plt.colorbar()
        plt.show()
        plt.close()
    
    
def clim_anomaly(crop_name,file_dir, shock_threshold = 1):
    #  Return  temp/soilmoisture anomalies over certain threshold
    
    scale_files = os.listdir(file_dir)
    scale_files = [file for file in scale_files if crop_name in file]
    # Lowest decile: 0.0_to_0.1, wetter/colder
    # Highest decile: 0.9_to_1.0, dryer/hotter
    
    clim_data= dict(zip (scale_files, [pickle.load(lzma.LZMAFile(file_dir + file_path)) for file_path in scale_files]))
    np.warnings.filterwarnings('ignore')
    # get only values over the shock threshold
    if shock_threshold:
        shock_threshold = np.float(shock_threshold)

    clim_sd = deepcopy(clim_data)
    for k, v in clim_sd.items():
        if shock_threshold:
            v[v<shock_threshold] = np.nan
        clim_sd[k] = v
    return clim_sd


def anom_normalized_yield(yield_data,yield_years, anom_data, analysis_unit, harv_area = None):
    
    
    yield_detrend = detrend_raster_data(yield_data, 5, relative_anom=True)
    
    yield_detrend = yield_detrend[:,:,yield_years]
    
    
    assert yield_detrend.shape == anom_data.shape
    
    normalized_yield = yield_detrend / anom_data
    
    if harv_area is not None:
        harv_area = harv_area[:,:,yield_years]
    
    
    yields = array_to_analysis_unit(
        array_in = normalized_yield, 
        analysis_unit= analysis_unit, 
        data_name ="year", 
        weights=harv_area)
    
    return yields

def detrend_raster_data(array_in, window_size, relative_anom = True):
    
    xr_array = xr.DataArray(array_in)
    roll_mean = xr_array.rolling(dim_2 = window_size,min_periods = 1).mean(skipna =True)
    
    detrend_data = array_in - roll_mean
    
    if relative_anom:
        detrend_data = detrend_data / roll_mean*100
        
    detrend_data = np.array(detrend_data)
        
    return detrend_data

 
def array_to_analysis_unit(array_in, analysis_unit, data_name,weights = None, isYears = True):
    analysis_unit = analysis_unit.reshape(-1)
    ids = np.unique(analysis_unit[analysis_unit>0])
    if len(array_in.shape) > 2:
        nlyr = array_in.shape[2]
    else:
        nlyr = 1
        array_in =array_in[:,:,np.newaxis]
    if isYears:
        lyr_name = 1981
    else:
        lyr_name =0
        
    if weights is None:
        weights = np.ones_like(array_in)
        #analysis_unit_out = {data_name + str(lyr_name + ii): \
        #np.bincount(analysis_unit, array_in[:,:,ii].reshape(-1))   \
        #for ii in range(0,nlyr)}
    #else:
    data_weighted = array_in * weights
  
    def summarise_to_analysis_unit(analysis_unit, array_in, w_in):
        vals = array_in.reshape(-1)
        is_nan = np.isnan(vals)
        vals[is_nan] = 0
        w = w_in.reshape(-1)
        w[is_nan] = 0
        #analysis_unit = analysis_unit[not_nan]
        
        vals_bin = np.bincount(analysis_unit,vals) [1:]
        weights_bin = np.bincount(analysis_unit, w) [1:]
        return (vals_bin / weights_bin)
  
    data_aunit = {data_name + str(lyr_name + ii): \
        summarise_to_analysis_unit(analysis_unit, 
        data_weighted[:,:,ii], weights[:,:,ii]) for ii in range(0,nlyr)}
    df_out = pd.DataFrame.from_dict(data_aunit)
    df_out.dropna(axis = 0, how="all",  inplace=True)
    df_out["area_id"] = df_out.index
    #df_out = df_out[df_out['area_id'].isin(ids)]
    return df_out
    
def clim_anom_outputs(base_path, analysis_unit_path, output_dir, shock_size = 1, crops =("maize","rice","wheat","soybean"), scale_anom = True ) :
    
    dest_folder = r'_data_1970To2013/'
    clim_anom_path = r'/Users/kinnunp4/Desktop/local_git_repos/shock_vulnerability/data/temp_moist_anom/'
    
    yield_timeperiod = np.linspace(1970,2013,44)
    anom_timeperiod = np.linspace(1981,2009,29)
    
    select_period = [year in anom_timeperiod for year in yield_timeperiod]
    
    analysis_unit = import_analysis_unit(path=analysis_unit_path,dims = (360,720))[0]
    
    df = pd.DataFrame()
    df_clim = pd.DataFrame()
    
    for crop_name in crops:
        #print(crop_name)
        # load data
        path_in = base_path + crop_name + dest_folder
        crop_yield =import_ray_crop_data(path_in, 'yield')
        harv_area = import_ray_crop_data(path_in, 'harvestedarea')
        clim_data = clim_anomaly(crop_name = crop_name, file_dir = clim_anom_path, shock_threshold = shock_size)
      
        anom_types = clim_data.keys()
        
        # crop yield corrected with shock size
        results_out = {}
        clim_df = {}
        for anom_type in anom_types:
            if scale_anom:
                anom_data = clim_data[anom_type]
            else:
                anom_data = clim_data[anom_type]>0
                
            results_out[anom_type]  = anom_normalized_yield(
                yield_data=crop_yield,
                yield_years= select_period,
                anom_data=anom_data,
                analysis_unit= analysis_unit,
                harv_area=harv_area)
                
            # count number of shocks for each anomaly+crop combination
            clim_temp = array_to_analysis_unit(array_in=clim_data[anom_type],analysis_unit=analysis_unit, data_name = "year", isYears=True)
            clim_temp["nshock"] = clim_temp.loc[:, clim_temp.columns.str.startswith('year')].apply(lambda x: [1 if y >1 else 0 for y in x]).sum(axis = 1)
            clim_df[anom_type] = clim_temp.loc[:, ["area_id","nshock"]]
         
        # Create an empty dataframe, this will be your final dataframe
        for key, sub_df in results_out.items():
           sub_df['variable_name'] = [key]*len(sub_df)
           sub_df['crop_name'] = crop_name
           df = df.append(sub_df, ignore_index=False) # Add your sub_df one by one
           
        for key, sub_df in clim_df.items():
           sub_df['variable_name'] = [key]*len(sub_df)
           sub_df['crop_name'] = crop_name
           df_clim =  df_clim.append(sub_df, ignore_index=False) # Add your sub_df one by one
           
    df_clim  = change_var_names(df_clim)
    df = change_var_names(df)
    
    if scale_anom:
        path_yield_clim_adjusted = os.path.join(output_dir,'crop_yields_clim_adjusted.feather')
        path2 =os.path.join(output_dir, 'crop_yields_clim_adjusted.csv')
    else:
        path_yield_clim_adjusted = os.path.join(output_dir,'crop_yields_anom_shocks.feather')
        path2 = os.path.join(output_dir, 'crop_yields_anom_shocks.csv')
    
    ft.write_dataframe(df, path_yield_clim_adjusted, version=1)
    df.to_csv(path2)
    
    ft.write_dataframe(df_clim,os.path.join(output_dir,'number_of_shocks.feather'), version=1)
    df_clim.to_csv(os.path.join(output_dir,'number_of_shocks.csv'))

def change_var_names(df):
  df['variable_name'] = df['variable_name'].map(lambda x: x.replace('detrended_anom_',''))
  df['variable_name'] = df['variable_name'].map(lambda x: x.replace('.pkl.lzma',''))
  df['variable'] = df['variable_name'].str.split('_90').str[0]
  df['shock_type'] = df['variable_name'].str.split("combined_").str[1]
  df.loc[(df.shock_type =='0.0_to_0.1') & (df.variable == 'temperature'), 'shock_type'] = 'cold'
  df.loc[(df.shock_type =='0.0_to_0.1') & (df.variable == 'soil_moisture_era'), 'shock_type'] = 'wet'
  df.loc[(df.shock_type =='0.9_to_1.0') & (df.variable == 'temperature'), 'shock_type'] = 'hot'
  df.loc[(df.shock_type =='0.9_to_1.0') & (df.variable == 'soil_moisture_era'), 'shock_type'] = 'dry'
  return df  


def yield_anom_outputs(base_path, analysis_unit_path, output_dir, is_relative_anom =True, crops =("maize","rice","wheat","soybean") ) :
    
    dest_folder = r'_data_1970To2013/'
    
    yield_timeperiod = np.linspace(1970,2013,44)
    anom_timeperiod = np.linspace(1981,2009,29)
    
    select_period = [year in anom_timeperiod for year in yield_timeperiod]
    
    analysis_unit = import_analysis_unit(path=analysis_unit_path,dims = (360,720))[0]
    
    df = pd.DataFrame()
    for crop_name in crops:
        # load data
        path_in = base_path + crop_name + dest_folder
        crop_yield = import_ray_crop_data(path_in, 'yield')
        harv_area = import_ray_crop_data(path_in, 'harvestedarea')
        
        detrend_crop_yield = detrend_raster_data(crop_yield, 5, relative_anom =is_relative_anom)
        detrend_crop_yield =detrend_crop_yield[:,:,select_period]
        harv_area = harv_area[:,:,select_period]
        
        analysis_unit_yield = array_to_analysis_unit(detrend_crop_yield, analysis_unit,"year", harv_area)
        analysis_unit_yield['crop_name'] = crop_name
        df = df.append(analysis_unit_yield, ignore_index=False) # Add your sub_df one by one
    
    if is_relative_anom:
        path_relative_anom = "relative"
    else:
        path_relative_anom = "detrended"
        
    path_yield = os.path.join(output_dir,path_relative_anom + '_crop_yields_anom.feather')
    path_yield_csv = os.path.join(output_dir,path_relative_anom + '_crop_yields_anom.csv')
    
    ft.write_dataframe(df, path_yield, version=1)
    df.to_csv(path_yield_csv)

def run_crop_data_import(path_data, path_aunit, output_path,shock_size = 1, crops_select = ("wheat","maize","rice", "soybean")):
    #analysis_unit_path = r'/Users/kinnunp4/Desktop/local_git_repos/shock_vulnerability/r/temp_data/analysis_unit.tif'
    #base_path = r'/Users/kinnunp4/Desktop/local_git_repos/shock_vulnerability/r/data/crop_data/'
    #output_dir  = r'/Users/kinnunp4/Desktop/local_git_repos/shock_vulnerability/r/temp_data/'    

    # Climate  adjusted yield anomaly
    clim_anom_outputs(path_data, path_aunit, output_path, shock_size = shock_size)
    # relative yield anomalies
    yield_anom_outputs(path_data, path_aunit, output_path)
    
    # detrended data from yields
    
    yield_anom_outputs(path_data, path_aunit, output_path, is_relative_anom = False)
    clim_anom_outputs(path_data, path_aunit, output_path, shock_size = shock_size, scale_anom=False)
    
    analysis_unit = import_analysis_unit(path_aunit, (360,720))[0]
    df_out = pd.DataFrame()
    
    df_harv_area = pd.DataFrame()
    
    df_clim_data = pd.DataFrame()
    clim_anom_path = '../data/temp_moist_anom/'
    clim_result ={}
    
    for crops in crops_select:
         crop_yield = import_ray_crop_data(path_data + crops + "_data_1970To2013/", "yield")
         harv_area = import_ray_crop_data(path_data + crops + "_data_1970To2013/", "harvestedarea")
         
         crop_yield = crop_yield[:,:,11:43]
         harv_area = harv_area[:,:,11:43]
         crops_to_unit = array_to_analysis_unit(crop_yield,analysis_unit=analysis_unit,data_name ="year", weights=harv_area)
         crops_to_unit["crop_name"] = crops
         
         harv_area_to_unit = array_to_analysis_unit(harv_area, analysis_unit=analysis_unit, data_name="year")
         harv_area_to_unit["crop_name"] = crops
         
         df_out = df_out.append(crops_to_unit)
         df_harv_area = df_harv_area.append(harv_area_to_unit)
    

    for crops in crops_select:
        clim_data =  clim_anomaly(crop_name = crops, file_dir = clim_anom_path, shock_threshold = shock_size)
        anom_types = clim_data.keys()
        # crop yield corrected with shock size
        
        for anom_type in anom_types:
            clim_data_to_unit = array_to_analysis_unit(clim_data[anom_type],analysis_unit=analysis_unit,data_name ="year")
            clim_result[anom_type] = clim_data_to_unit
            
        for key, sub_df in clim_result.items():
            sub_df['variable_name'] = [key]*len(sub_df)
            sub_df['crop_name'] = crops
            df_clim_data = df_clim_data.append(sub_df, ignore_index=False) # Add your sub_df one by one
           
    
    df_clim_data['variable_name'] = df_clim_data['variable_name'].map(lambda x: x.replace('detrended_anom_',''))
    df_clim_data['variable_name'] = df_clim_data['variable_name'].map(lambda x: x.replace('.pkl.lzma',''))
    df_clim_data['variable'] = df_clim_data['variable_name'].str.split('_90').str[0]
    df_clim_data['shock_type'] = df_clim_data['variable_name'].str.split("combined_").str[1]
    df_clim_data.loc[(df_clim_data.shock_type =='0.0_to_0.1') & (df_clim_data.variable == 'temperature'), 'shock_type'] = 'cold'
    df_clim_data.loc[(df_clim_data.shock_type =='0.0_to_0.1') & (df_clim_data.variable == 'soil_moisture_era'), 'shock_type'] = 'wet'
    df_clim_data.loc[(df_clim_data.shock_type =='0.9_to_1.0') & (df_clim_data.variable == 'temperature'), 'shock_type'] = 'hot'
    df_clim_data.loc[(df_clim_data.shock_type =='0.9_to_1.0') & (df_clim_data.variable == 'soil_moisture_era'), 'shock_type'] = 'dry'
    
    
    
    ft.write_dataframe(df_clim_data, os.path.join(output_path, "climate_anomalies.feather"), version=1)
    ft.write_dataframe(df_harv_area, os.path.join(output_path, "harvested_area.feather"), version=1)
    
    df_clim_data.to_csv(os.path.join(output_path, "climate_anomalies.csv"))
    df_harv_area.to_csv(os.path.join(output_path, "harvested_area.csv"))
    


def climate_anomalies_out(path_data, path_aunit, output_path,shock_size = -2, crops_select = ("wheat","maize","rice", "soybean")):
  analysis_unit_path = r'/Users/kinnunp4/Desktop/local_git_repos/shock_vulnerability/r/temp_data/analysis_unit.tif'
  analysis_unit = import_analysis_unit(path_aunit, (360,720))[0]
  analysis_unit = import_analysis_unit(analysis_unit_path, (360,720))[0]
  df_clim_data = pd.DataFrame()
  clim_anom_path = '../data/temp_moist_anom/'
  
  for crops in crops_select:
      
      clim_result ={}
      clim_data =  clim_anomaly(crop_name = crops, file_dir = clim_anom_path, shock_threshold = shock_size)
      anom_types = clim_data.keys()
      
      for anom_type in anom_types:
          clim_data_to_unit = array_to_analysis_unit(clim_data[anom_type],analysis_unit=analysis_unit,data_name ="year")
          clim_result[anom_type] = clim_data_to_unit
          
      for key, sub_df in clim_result.items():
        print(key)
        sub_df['variable_name'] = [key]*len(sub_df)
        sub_df['crop_name'] = crops
        df_clim_data = df_clim_data.append(sub_df, ignore_index=False) # Add your sub_df one by one
        
  df_clim_data = change_var_names(df_clim_data)
  ft.write_dataframe(df_clim_data, os.path.join(output_path, "all_climate_anomalies.feather"), version=1)
  df_clim_data.to_csv(os.path.join(output_path, "all_climate_anomalies.csv"))

  
